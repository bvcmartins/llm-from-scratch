{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea4b882",
   "metadata": {},
   "source": [
    "Source: https://wandb.ai/byyoung3/Generative-AI/reports/How-to-fine-tune-and-evaluate-Qwen3-with-Unsloth---VmlldzoxMjU3OTI0Ng#fine-tuning-qwen3-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from unsloth import FastLanguageModel\n",
    "import weave\n",
    "from pprint import pprint\n",
    "weave.init('think_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d931a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_MODEl_NAME = \"unsloth/Qwen3-8B\"\n",
    "BASE_MODEl_NAME = \"unsloth/Qwen3-0.6B\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = False\n",
    "\n",
    "BASE_MODEL, TOKENIZER = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEl_NAME,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01799a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL.eval().to(\"cuda\")\n",
    "FastLanguageModel.for_inference(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3832e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(instruction):\n",
    "    return [{\"role\": \"user\", \"content\": instruction}]\n",
    "\n",
    "def apply_chat_template(prompt, tokenizer, enable_thinking=True):\n",
    "    messages = make_prompt(prompt)\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b5397",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def generate_response(prompt, enable_thinking=True):\n",
    "    prompt_text = apply_chat_template(prompt, TOKENIZER, enable_thinking)\n",
    "    inputs = TOKENIZER([prompt_text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        gen_output = BASE_MODEL.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=1000,\n",
    "            use_cache=False, \n",
    "            temperature=0.7,\n",
    "            top_p=0.8,\n",
    "            top_k=20,\n",
    "            min_p=0.0\n",
    "        )\n",
    "    output_text = TOKENIZER.decode(gen_output[0], skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d641383",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_question = \"What is 256 multiplied by 17?\"\n",
    "math_question_no_think = \"/no_think\\nWhat is 256 multiplied by 17?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== enable_thinking=True (default) ===\")\n",
    "output1 = generate_response(math_question, enable_thinking=True)\n",
    "pprint(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e3459",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== enable_thinking=False ===')\n",
    "output2 = generate_response(math_question, enable_thinking=False)\n",
    "pprint(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd453b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== enable_thinking=True + /no_think in prompt ===')\n",
    "output3 = generate_response(math_question_no_think, enable_thinking=True)\n",
    "pprint(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "SEED = 3278\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True \n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del BASE_MODEL, TOKENIZER\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beebfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bf16_supported\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True  # Changed from False to True\n",
    "MODEL_NAME = \"unsloth/Qwen3-0.6B\"\n",
    "SAVE_DIR = \"lora_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b1ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model, \n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=SEED,\n",
    "    use_rslora=False, \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ebf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        if input_text.strip():\n",
    "            user_message = f\"{instruction}\\n\\n{input_text}\"\n",
    "        else:\n",
    "            user_message = instruction \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": output}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            enable_thinking=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0db892",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "half_len = len(dataset) // 2\n",
    "dataset = dataset.select(range(half_len))\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccaec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2366276",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2, \n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bf16_supported(),\n",
    "        bf16=is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\", \n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\", \n",
    "        seed=SEED,\n",
    "        output_dir=\"outputs\", \n",
    "        report_to=\"none\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c471f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "user_query = \"Continue the Fibonacci sequence.\\n\\n1, 1, 2, 3, 5, 8\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_query}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "inputs = tokenizer([prompt], return_tensors='pt').to('cuda')\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    use_cache=False,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "    min_p=0.0\n",
    ")\n",
    "\n",
    "pprint(\"\\n============= Output from in-memory model (just trained)\")\n",
    "pprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=SAVE_DIR,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "\n",
    "inputs2 = tokenizer([prompt2], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs2 = model.generate(\n",
    "    **inputs2,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=False,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "    min_p=0.0\n",
    ")\n",
    "\n",
    "print(\"\\n============== Output from reloaded model (after save / load)\")\n",
    "print(tokenizer.decode(outputs2[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3710cc3b",
   "metadata": {},
   "source": [
    "### Evaluate with Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "SEED = 3407\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True \n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "BASE_MODEL_NAME = \"unsloth/Qwen3-0.6B\"\n",
    "LORA_MODEL_DIR = \"lora_model\"\n",
    "N = 30\n",
    "weave.init(\"q3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f922234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GLOBAL: LOAD MODELS ONLY ONCE ===\n",
    "BASE_MODEL, TOKENIZER = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_NAME, \n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")\n",
    "\n",
    "LORA_MODEL, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=LORA_MODEL_DIR,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")\n",
    "\n",
    "BASE_MODEL.eval()\n",
    "LORA_MODEL.eval()\n",
    "FastLanguageModel.for_inference(BASE_MODEL)\n",
    "FastLanguageModel.for_inference(LORA_MODEL)\n",
    "\n",
    "def make_prompt(instruction, input_text):\n",
    "    if input_text.strip():\n",
    "        user_message = f\"{instruction}\\n\\n{input_text}\"\n",
    "    else:\n",
    "        user_message = instruction\n",
    "    return [{\"role\": \"user\", \"content\": user_message}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template_loss(sample, tokenizer):\n",
    "    messages = make_prompt(sample[\"instruction\"], sample[\"input\"])\n",
    "    messages.append({\"role\": \"assistant\", \"content\": sample[\"output\"]})\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template_generation(sample, tokenizer):\n",
    "    messages = make_prompt(sample[\"instruction\"], sample[\"input\"])\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_only_loss(toknizer, model, sample, device=\"cuda\"):\n",
    "    #1. Prepare full prompt + output for loss \n",
    "    prompt_plus_output = apply_chat_template_loss(sample, tokenizer)\n",
    "    #2. Prepare prompt only (for prefix length)\n",
    "    prompt_only = make_prompt(sample[\"instruction\"], sample[\"input\"])\n",
    "    prompt_only_str = tokenizer.apply_chat_template(\n",
    "        prompt_only,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False, \n",
    "        enable_thinking=False\n",
    "    )\n",
    "    #3. Toknize both \n",
    "    tok_full = tokenizer(\n",
    "        prompt_plus_output,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    tok_prompt = tokenizer(\n",
    "        prompt_only_str,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "    input_ids = tok_full[\"input_ids\"].to(device)\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # 4 Loss only on output tokens \n",
    "    prompt_len = tok_prompt[\"input_ids\"].shape[-1]\n",
    "    # mask pad tokens if there \n",
    "    labels[:, :prompt_len] = -100\n",
    "    if tokenizer.pad_token_id is not None: \n",
    "        labels[input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids, labels=labels)\n",
    "    return output.loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01767de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_generate(model, tokenizer, prompt, device=\"cuda\"):\n",
    "    # Tokenize prompt and ensure we never overflow model max length \n",
    "    prompt_tok = tokenizer(\n",
    "        [prompt], \n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    ).to(device)\n",
    "    prompt_len = prompt_tok['input_ids'].shape[1]\n",
    "    # prevent overflow: at least generate 1, never beyond 2048\n",
    "    max_gen = max(1, max_seq_length - prompt_len)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **prompt_tok,\n",
    "            max_new_tokens=max_gen,\n",
    "            use_cache=False,\n",
    "            temperature=0.7,\n",
    "            top_p=0.8,\n",
    "            top_k=20,\n",
    "            min_p=0.0\n",
    "        )\n",
    "        out_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return out_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenBaseModel(weave.Model):\n",
    "    @weave.op()\n",
    "    async def predict(self, instruction, input, output):\n",
    "        sample = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input,\n",
    "            \"output\": output\n",
    "        }\n",
    "        # loss on output tokens only \n",
    "        loss = output_only_loss(TOKENIZER, BASE_MODEL, sample)\n",
    "        prompt_gen = apply_chat_template_generation(sample, TOKENIZER)\n",
    "        output_text = safe_generate(BASE_MODEL, TOKENIZER, prompt_gen)\n",
    "        return {\"loss\": loss, \"output\": output_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ee3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenLoraModel(weave.Model):\n",
    "    @weave.op()\n",
    "    async def predict(self, instruction, input, output):\n",
    "        sample = {\n",
    "            \"instruction\": instruction, \n",
    "            \"input\": input, \n",
    "            \"output\": output\n",
    "        }\n",
    "        loss = output_only_loss(TOKENIZER, LORA_MODEL, sample)\n",
    "        prompt_gen = apply_chat_template_generation(sample, TOKENIZER)\n",
    "        output_text = safe_generate(LORA_MODEL, TOKENIZER, prompt_gen)\n",
    "        return {\"loss\": loss, \"output\": output_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4eaf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def loss_only_scorer(output):\n",
    "    return {\"loss\": output[\"loss\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd8f5cc",
   "metadata": {},
   "source": [
    "### Load last 10% of train and pick 30 samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "length = len(full_ds)\n",
    "start = int(length * 0.9)\n",
    "end = length \n",
    "ds_last10 = full_ds.select(range(start, end))\n",
    "samples = [\n",
    "    dict(\n",
    "        instruction=row[\"instruction\"],\n",
    "        input=row[\"input\"], \n",
    "        output=row[\"output\"]\n",
    "    )\n",
    "    for row in ds_last10.select(range(N))\n",
    "]\n",
    "\n",
    "async def main():\n",
    "    models = {\n",
    "        \"Qwen3-8B-base\": QwenBaseModel(),\n",
    "        \"Qwen3-8B-LoRA\": QwenLoraModel()\n",
    "    }\n",
    "    scorers = [loss_only_scorer]\n",
    "    for model_name, model, in models.items():\n",
    "        print(f\"==== Evaluating {model_name} ====\")\n",
    "        evaluation = weave.Evaluation(\n",
    "            dataset=samples, \n",
    "            scorers=scorers, \n",
    "            name=f\"{model_name} LossEval\"\n",
    "        )\n",
    "        results = await evaluation.evaluate(model)\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ac35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356fa6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
