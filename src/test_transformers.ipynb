{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d731d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU) is available.\n",
      "Number of GPUs: 1\n",
      "Current GPU device: 0\n",
      "GPU Name: NVIDIA GeForce RTX 5090 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmartins/anaconda3/envs/agpat2/lib/python3.11/site-packages/torch/cuda/__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5090 Laptop GPU with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_89 sm_90 compute_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5090 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU) is available.\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\") # Get name of the first GPU\n",
    "else:\n",
    "    print(\"CUDA (GPU) is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b42ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f6bbdfb7644ef282301b9b84f6dbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen3-14B\"\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4678d214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,    872,    198,  35127,    752,    264,   2805,  16800,    311,\n",
       "           3460,   4128,   1614,     13, 151645,    198, 151644,  77091,    198]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8f4020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151667,\n",
       " 198,\n",
       " 32313,\n",
       " 11,\n",
       " 279,\n",
       " 1196,\n",
       " 374,\n",
       " 10161,\n",
       " 369,\n",
       " 264,\n",
       " 2805,\n",
       " 16800,\n",
       " 311,\n",
       " 3460,\n",
       " 4128,\n",
       " 4119,\n",
       " 13,\n",
       " 6771,\n",
       " 752,\n",
       " 1191,\n",
       " 553,\n",
       " 26312,\n",
       " 1128,\n",
       " 264,\n",
       " 3460,\n",
       " 4128,\n",
       " 1614,\n",
       " 374,\n",
       " 13,\n",
       " 358,\n",
       " 1265,\n",
       " 6286,\n",
       " 429,\n",
       " 807,\n",
       " 2299,\n",
       " 15235,\n",
       " 5942,\n",
       " 16176,\n",
       " 389,\n",
       " 12767,\n",
       " 14713,\n",
       " 315,\n",
       " 1467,\n",
       " 821,\n",
       " 13,\n",
       " 10696,\n",
       " 10339,\n",
       " 862,\n",
       " 1376,\n",
       " 4419,\n",
       " 1075,\n",
       " 8660,\n",
       " 323,\n",
       " 23163,\n",
       " 3738,\n",
       " 12681,\n",
       " 1467,\n",
       " 382,\n",
       " 40,\n",
       " 1184,\n",
       " 311,\n",
       " 3421,\n",
       " 862,\n",
       " 8357,\n",
       " 11,\n",
       " 1075,\n",
       " 35764,\n",
       " 4755,\n",
       " 11,\n",
       " 4378,\n",
       " 7343,\n",
       " 11,\n",
       " 476,\n",
       " 10822,\n",
       " 13,\n",
       " 7281,\n",
       " 11,\n",
       " 432,\n",
       " 594,\n",
       " 2989,\n",
       " 311,\n",
       " 5185,\n",
       " 862,\n",
       " 16928,\n",
       " 11,\n",
       " 1741,\n",
       " 438,\n",
       " 2745,\n",
       " 49823,\n",
       " 1824,\n",
       " 323,\n",
       " 11589,\n",
       " 6351,\n",
       " 9079,\n",
       " 13,\n",
       " 1988,\n",
       " 358,\n",
       " 13133,\n",
       " 944,\n",
       " 10667,\n",
       " 311,\n",
       " 6286,\n",
       " 279,\n",
       " 11513,\n",
       " 11,\n",
       " 1075,\n",
       " 54480,\n",
       " 4963,\n",
       " 323,\n",
       " 4650,\n",
       " 49083,\n",
       " 13,\n",
       " 13655,\n",
       " 432,\n",
       " 63594,\n",
       " 714,\n",
       " 38219,\n",
       " 13,\n",
       " 6771,\n",
       " 752,\n",
       " 5944,\n",
       " 432,\n",
       " 1119,\n",
       " 264,\n",
       " 2421,\n",
       " 2797,\n",
       " 3501,\n",
       " 2041,\n",
       " 3709,\n",
       " 2238,\n",
       " 10916,\n",
       " 624,\n",
       " 151668,\n",
       " 271,\n",
       " 32,\n",
       " 3070,\n",
       " 16767,\n",
       " 4128,\n",
       " 1614,\n",
       " 320,\n",
       " 4086,\n",
       " 44,\n",
       " 32295,\n",
       " 374,\n",
       " 458,\n",
       " 10847,\n",
       " 20443,\n",
       " 11229,\n",
       " 1849,\n",
       " 16176,\n",
       " 389,\n",
       " 12767,\n",
       " 14713,\n",
       " 315,\n",
       " 1467,\n",
       " 821,\n",
       " 311,\n",
       " 3535,\n",
       " 323,\n",
       " 6923,\n",
       " 3738,\n",
       " 12681,\n",
       " 4128,\n",
       " 13,\n",
       " 4220,\n",
       " 4119,\n",
       " 24538,\n",
       " 518,\n",
       " 9079,\n",
       " 1075,\n",
       " 35764,\n",
       " 4755,\n",
       " 11,\n",
       " 4378,\n",
       " 7343,\n",
       " 11,\n",
       " 10822,\n",
       " 11,\n",
       " 66271,\n",
       " 15459,\n",
       " 11,\n",
       " 323,\n",
       " 803,\n",
       " 11,\n",
       " 553,\n",
       " 48083,\n",
       " 12624,\n",
       " 323,\n",
       " 2266,\n",
       " 304,\n",
       " 1467,\n",
       " 13,\n",
       " 2379,\n",
       " 525,\n",
       " 5798,\n",
       " 1667,\n",
       " 5538,\n",
       " 6832,\n",
       " 12538,\n",
       " 11,\n",
       " 3545,\n",
       " 448,\n",
       " 32051,\n",
       " 315,\n",
       " 5029,\n",
       " 11,\n",
       " 27362,\n",
       " 1105,\n",
       " 311,\n",
       " 3705,\n",
       " 6351,\n",
       " 19556,\n",
       " 323,\n",
       " 8193,\n",
       " 55787,\n",
       " 11,\n",
       " 2266,\n",
       " 1832,\n",
       " 9760,\n",
       " 14507,\n",
       " 13,\n",
       " 444,\n",
       " 10994,\n",
       " 82,\n",
       " 525,\n",
       " 88228,\n",
       " 311,\n",
       " 6481,\n",
       " 15235,\n",
       " 8357,\n",
       " 11,\n",
       " 504,\n",
       " 6236,\n",
       " 61905,\n",
       " 311,\n",
       " 2213,\n",
       " 9688,\n",
       " 11,\n",
       " 3498,\n",
       " 807,\n",
       " 1373,\n",
       " 5089,\n",
       " 54480,\n",
       " 4963,\n",
       " 323,\n",
       " 16585,\n",
       " 41338,\n",
       " 311,\n",
       " 29337,\n",
       " 49083,\n",
       " 323,\n",
       " 5978,\n",
       " 13403,\n",
       " 13,\n",
       " 151645]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883fa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmartins/anaconda3/envs/agpat2/lib/python3.11/site-packages/torch/cuda/__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5090 Laptop GPU with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_89 sm_90 compute_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5090 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79357cb7fb77404ba307469c2fe46012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user wants a short introduction to large language models. Let me start by defining what they are. I should mention that they're AI models trained on vast amounts of text data. Maybe explain their capabilities, like generating text, answering questions, and understanding context.\n",
      "\n",
      "I need to highlight their applicationsâ€”like chatbots, content creation, and translation. Also, it's important to note their ability to handle multiple languages and adapt to different tasks. But I should keep it concise. Don't forget to mention the underlying technology, such as deep learning and neural networks. Make sure it's easy to understand without too much jargon. Let me check if I covered the key points: definition, training data, capabilities, applications, and technology. Yeah, that should work. Keep it under a paragraph or two.\n",
      "</think>\n",
      "content: A **large language model (LLM)** is an advanced artificial intelligence system trained on vast amounts of text data to understand and generate human-like language. These models use deep learning techniques, such as transformer architectures, to process and predict sequences of words, enabling them to perform tasks like answering questions, writing text, translating languages, and more. LLMs excel at understanding context, adapting to diverse topics, and producing coherent, contextually relevant responses. They power applications ranging from chatbots and content creation to research assistance and code generation, making them versatile tools in both personal and professional settings.\n"
     ]
    }
   ],
   "source": [
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agpat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
